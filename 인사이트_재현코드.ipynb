{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  팀 인사이트 모델 코드 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 개발환경 및 라이브러리 정보 </center>\n",
    "\n",
    "모듈(OS) | 버전\n",
    "--------- | ---------\n",
    "OS | macOS Catalina\n",
    "python & 내장 모듈 | 3.8.3 64-bit\n",
    "tqdm | 4.56.0\n",
    "pandas | 1.2.1\n",
    "numpy | 1.20.0\n",
    "catboost | 0.24.4\n",
    "sklearn | 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>목차</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Index`\n",
    "    - **1. 데이터셋 준비**\n",
    "        - 1-1. 라이브러리 / 데이터 로드  \n",
    "        - 1-2. 결측치 처리\n",
    "    - **2. 데이터 전처리**\n",
    "        - 2-1. 변수 처리\n",
    "        - 2-2. 파생변수 생성\n",
    "            - 2-2-1. make_dataset1 : 에러타입, 주요 에러코드 관련 변수\n",
    "            - 2-2-2. make_dataset2 : 단위시간 별 에러타입/퀄리티 관련 변수\n",
    "            - 2-2-3. make_dataset3 : 주로 펌웨어 버전, 모델 관련 변수\n",
    "            - 2-2-4. 최종 데이터 병합 <br>\n",
    "    - **3. 모델 훈련 및 예측**\n",
    "        - 3-1. 모델 학습\n",
    "        - 3-2. 예측 및 제출파일 생성 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>1. 데이터셋 준비</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 라이브러리 / 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime as dt\n",
    "import collections\n",
    "import random\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "DATA_PATH = \"./preprocessing/\"\n",
    "SAVE_PATH = \"./preprocessing/\"\n",
    "RESULT_PATH = \"./result/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 데이터\n",
    "- train_err : train_err_data\n",
    "- train_qual : train_quality_data\n",
    "- train_problem : train_problem_data  \n",
    "<br>\n",
    "- test_err : test_err_data\n",
    "- test_qual : test_quality_data  \n",
    "<br>\n",
    "- problem_user : 불만을 제기한 고객들\n",
    "- no_problem_user : 불만을 제기안한 고객들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_err = pd.read_csv(DATA_PATH+ 'train_err_data.csv')\n",
    "train_qual = pd.read_csv(DATA_PATH+\"train_quality_data.csv\")\n",
    "train_problem = pd.read_csv(DATA_PATH+ 'train_problem_data.csv')\n",
    "\n",
    "test_err = pd.read_csv(DATA_PATH+\"test_err_data.csv\")\n",
    "test_qual = pd.read_csv(DATA_PATH+'test_quality_data.csv')\n",
    "\n",
    "# 불만 접수 고객 확인\n",
    "problem_user = train_problem.user_id.unique()\n",
    "no_problem_user = list(set(train_err.user_id.unique())  - set(problem_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "random.seed(1422)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 결측치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 인덱스 슬라이싱 : EDA로 확인 후 최빈값으로 대체 \n",
    "train_err.iloc[3825744,5] = train_err.errcode.mode()[0]\n",
    "\n",
    "test_err.iloc[937967,5] = test_err.errcode.mode()[0]\n",
    "test_err.iloc[4038892,5] = test_err.errcode.mode()[0]\n",
    "test_err.iloc[9486881,5] = test_err.errcode.mode()[0]\n",
    "test_err.iloc[10425473,5] = test_err.errcode.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퀄리티 데이터는 0으로 대체\n",
    "train_qual = train_qual.fillna(0)\n",
    "test_qual = test_qual.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>2. 데이터 전처리</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. 변수 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 기본 정보\n",
    "train_user_id_max = 24999\n",
    "train_user_id_min = 10000\n",
    "train_user_number = 15000\n",
    "\n",
    "test_user_id_max = 44998\n",
    "test_user_id_min = 30000\n",
    "test_user_number = 14999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pre_funcs : 데이터 전처리 함수들의 집합\n",
    "  - make_datetime : 날짜 데이터를 datetime type으로 변환\n",
    "  - get_total : 각 변수 별 모든 종류 리스트 반환\n",
    "  - make_day : 일(date) 추가\n",
    "  - make_hour : 시(hour) 추가\n",
    "  - str2int : str 타입을 int 타입으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pre_funcs() : # 데이터 전처리 함수들\n",
    "    \n",
    "    def make_datetime(x): # datetime 데이터로 변환\n",
    "        x     = str(x)\n",
    "        year  = int(x[:4])\n",
    "        month = int(x[4:6])\n",
    "        day   = int(x[6:8])\n",
    "        hour  = int(x[8:10])\n",
    "        minute  = int(x[10:12])\n",
    "        sec  = int(x[12:])\n",
    "        return dt.datetime(year, month, day, hour,minute,sec)\n",
    "    \n",
    "    def get_total(col,df1,df2): # 각 데이터별 모든 종류구하기\n",
    "        train_unique = set(df1[col].unique())\n",
    "        test_unique  = set(df2[col].unique())\n",
    "        total = (train_unique | test_unique) \n",
    "        return total\n",
    "    \n",
    "    def make_day(col): # 일자 반환\n",
    "        col = str(col)\n",
    "        year  = int(col[:4])\n",
    "        month = int(col[4:6])\n",
    "        day   = int(col[6:8])\n",
    "\n",
    "        if month == 10:\n",
    "            return 0\n",
    "        elif month == 11:\n",
    "            return day\n",
    "        else:\n",
    "            return 32\n",
    "    \n",
    "    def make_hour(col): # 시 반환\n",
    "        return int(str(col)[8:10])\n",
    "    \n",
    "    def str2int(x): # string -> int\n",
    "        if type(x) == str:\n",
    "            x = x.replace(\",\",\"\")\n",
    "            x = float(x)\n",
    "            return int(x)\n",
    "        else:\n",
    "            return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파생변수 목적에 맞게 각기 다른 전처리 : err1 -> mk_dataset1 \n",
    "train_err1 = train_err.copy()\n",
    "train_err1['date_k'] = train_err1['time'].apply(lambda x:pre_funcs.make_day(x))\n",
    "train_err1['hour'] = train_err1['time'].apply(lambda x:pre_funcs.make_hour(x))\n",
    "\n",
    "test_err1 = test_err.copy()\n",
    "test_err1['date_k'] = test_err1['time'].apply(lambda x:pre_funcs.make_day(x))\n",
    "test_err1['hour'] = test_err1['time'].apply(lambda x:pre_funcs.make_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파생변수 목적에 맞게 각기 다른 전처리 : err2 -> mk_dataset3\n",
    "train_err2 = train_err.copy()\n",
    "test_err2 = test_err.copy()\n",
    "\n",
    "train_err2.time = train_err2.time.apply(lambda x: pre_funcs.make_datetime(x))\n",
    "test_err2.time = test_err2.time.apply(lambda x: pre_funcs.make_datetime(x))\n",
    "\n",
    "model_total= pre_funcs.get_total('model_nm',train_err2,test_err2)\n",
    "errtype_total=pre_funcs.get_total('errtype',train_err2,test_err2)\n",
    "fwver_total = pre_funcs.get_total('fwver',train_err2,test_err2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality \n",
    "\n",
    "# qual1 : train \n",
    "qual1 = train_qual.copy()\n",
    "qual1['date'] = qual1['time'].apply(lambda x: int(str(x)[:8]))\n",
    "list_qual = list(qual1.columns[3:-1])\n",
    "for i in list_qual:\n",
    "    qual1[i] = qual1[i].apply(lambda x: pre_funcs.str2int(x))\n",
    "    \n",
    "# qual2 : test\n",
    "qual2 = test_qual.copy()\n",
    "qual2['date'] = qual2['time'].apply(lambda x: int(str(x)[:8]))\n",
    "list_qual2 = list(qual2.columns[3:-1])\n",
    "for i in list_qual:\n",
    "    qual2[i] = qual2[i].apply(lambda x: pre_funcs.str2int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 파생변수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-1. make_dataset1\n",
    "  - run : 전체 함수 실행\n",
    "  - make_errcode_list : 주요 에러코드 리스트 생성\n",
    "  - df_make : 에러타입/에러코드마다 카운트, 날짜 및 시간대 별 전체 에러타입 발생횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset1 : # 에러타입/ 에러코드 관련 파생변수 생성 \n",
    "    \n",
    "    def __init__(self, df, types, num_user, min_idx):\n",
    "        self.df = df\n",
    "        self.types = types\n",
    "        self.num_user = num_user\n",
    "        self.min_idx = min_idx\n",
    "        \n",
    "    def run(self) : # 전체 실행 함수\n",
    "        self.critical_errcode = self.make_errcode_list()\n",
    "        result = self.df_make()\n",
    "        return result\n",
    "        \n",
    "    def make_errcode_list (self): # 주요 에러코드 리스트 생성 함수\n",
    "        label = np.zeros(15000)\n",
    "        label[train_problem.user_id.unique()-10000] = 1 \n",
    "\n",
    "        problem_list = list(problem_user)\n",
    "        problem_code= train_err[train_err['user_id'].isin(problem_list)]['errcode'].value_counts().to_dict()\n",
    "        no_problem_code = train_err[~train_err['user_id'].isin(problem_list)]['errcode'].value_counts().to_dict()\n",
    "\n",
    "        total_code = []\n",
    "        critical_errcode = []\n",
    "        for i in problem_code:\n",
    "            try :\n",
    "                total_code.append((i,problem_code[i]/(problem_code[i]+no_problem_code[i]),problem_code[i],no_problem_code[i]))\n",
    "            except :\n",
    "                total_code.append((i,1,problem_code[i],0))\n",
    "\n",
    "        for i in no_problem_code:\n",
    "            if i in problem_code:\n",
    "                continue\n",
    "            total_code.append((i,0,0,no_problem_code[i]))\n",
    "\n",
    "        total_code.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "\n",
    "        for i in total_code:\n",
    "            if i[2] >= 5 or i[3] >= 5:\n",
    "                critical_errcode.append(i[0])\n",
    "        return critical_errcode\n",
    "\n",
    "    def df_make (self) : # 에러코드, 날짜, 시간대 데이터 생성 함수\n",
    "        num_user = self.num_user\n",
    "        min_idx = self.min_idx\n",
    "        clist = self.critical_errcode\n",
    "\n",
    "        id_error = self.df[['user_id','errtype','errcode', 'fwver', 'date_k', 'hour']].values\n",
    "        error = np.zeros((num_user,223))\n",
    "        vers = [' '] * num_user\n",
    "\n",
    "        # 0~ 41 : 에러타입, 42~165: 에러코드분류, 166~198: 날짜, 199~222 : 시간대\n",
    "        for person_idx, err, code, v, d, h in tqdm(id_error):\n",
    "            error[person_idx - min_idx,err - 1] += 1\n",
    "\n",
    "            if code in clist :\n",
    "                error[person_idx - min_idx,42+clist.index(code)] += 1\n",
    "\n",
    "            vers[person_idx - min_idx] = v\n",
    "\n",
    "            error[person_idx - min_idx,166 + d ] += 1\n",
    "            error[person_idx - min_idx,199 + h ] += 1\n",
    "\n",
    "        res = pd.DataFrame(data=error, columns = \n",
    "                ['err_' + str(i+1) + '_cnt' for i in range(42)]+\n",
    "                ['err_' + str(i+1) + '_code' for i in range(124)]+\n",
    "                ['err_' + str(i+1) + '_date' for i in range(33)]+\n",
    "                ['err_' + str(i+1) + '_hour' for i in range(24)])\n",
    "        res['Total'] = res.sum(axis=1)\n",
    "        res['ver'] = vers\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = make_dataset1(train_err1,'train', 15000, 10000).run()\n",
    "test1 = make_dataset1(test_err1,'test', 14999, 30000).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-2. make_dataset2\n",
    "  - make_qual_data : 퀄리티 관련 함수들 모두 실행\n",
    "  - move_errtype : 단위 기간별(3일,7일) 에러타입 통계수치\n",
    "  - qual_cnt : 퀄리티마다 단순합\n",
    "  - make_id_in_qual : 퀄리티 데이터에 없는 id 채우기\n",
    "  - move_quality : 12행마다 퀄리티 관련 통계수치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class make_dataset2():# 에러타입 파생변수, 퀄리티 파생변수 생성\n",
    "    def __init__(self, df, scope, num_user, min_idx):\n",
    "        self.df = df\n",
    "        self.scope = scope\n",
    "        self.num_user = num_user\n",
    "        self.min_idx = min_idx\n",
    "    \n",
    "    def make_qual_data(self): # 퀄리티 관련 함수 실행\n",
    "        qual_plus = self.qual_cnt()\n",
    "        \n",
    "        qual_tmp = self.make_id_in_qual()\n",
    "        qual_tmp0 = self.move_quality(qual_tmp, 0, moving=12)\n",
    "        qual_tmp1 = self.move_quality(qual_tmp, 1, moving=12)\n",
    "        qual_tmp2 = self.move_quality(qual_tmp, 2, moving=12)\n",
    "        qual_tmp3 = self.move_quality(qual_tmp, 3, moving=12)\n",
    "        qual_tmp4 = self.move_quality(qual_tmp, 4, moving=12)\n",
    "        qual_tmp5 = self.move_quality(qual_tmp, 5, moving=12)\n",
    "        qual_tmp6 = self.move_quality(qual_tmp, 6, moving=12)\n",
    "        qual_tmp7 = self.move_quality(qual_tmp, 7, moving=12)\n",
    "        qual_tmp8 = self.move_quality(qual_tmp, 8, moving=12)\n",
    "        qual_tmp9 = self.move_quality(qual_tmp, 9, moving=12)\n",
    "        qual_tmp10 = self.move_quality(qual_tmp, 10, moving=12)\n",
    "        qual_tmp11 = self.move_quality(qual_tmp, 11, moving=12)\n",
    "        qual_tmp12 = self.move_quality(qual_tmp, 12, moving=12)\n",
    "        \n",
    "        return pd.concat([qual_plus, qual_tmp0, qual_tmp1, qual_tmp2, qual_tmp3, \n",
    "                        qual_tmp4, qual_tmp5, qual_tmp6, qual_tmp7, qual_tmp8, qual_tmp9, qual_tmp10,\n",
    "                        qual_tmp11, qual_tmp12], axis=1).fillna(0)\n",
    "    \n",
    "    \n",
    "    def move_errtype(self): # 단위 시간별 에러타입 통계수치\n",
    "        tqdm.pandas()\n",
    "        data = self.df\n",
    "        data2 = data.copy()\n",
    "        scope= self.scope\n",
    "        data2['date'] = data['time'].apply(lambda x: int(str(x)[:8]))\n",
    "\n",
    "        # user_id, date, errtype\n",
    "        errtype_array = np.zeros((self.num_user, 30, 42))\n",
    "        user_id_min = min(data2['user_id'])\n",
    "\n",
    "        def myfunc(x):\n",
    "            for day in range(x['date'] % 100, min(31, x['date'] % 100 + scope)):\n",
    "                errtype_array[x['user_id'] - user_id_min, day-1, x['errtype']-1] += 1\n",
    "\n",
    "        data2.progress_apply(myfunc, axis=1)\n",
    "\n",
    "        errtype_array_max = errtype_array.max(axis=1)\n",
    "        errtype_array_min = errtype_array.min(axis=1)\n",
    "        errtype_array_std = errtype_array.std(axis=1)\n",
    "\n",
    "        return pd.concat([pd.DataFrame(data=errtype_array_max, columns=['err_' + str(i+1) + \n",
    "                                                                        '_move_' + str(scope) + \n",
    "                                                                        '_max' for i in range(42)]),\n",
    "                          pd.DataFrame(data=errtype_array_min, columns=['err_' + str(i+1) + \n",
    "                                                                        '_move_' + str(scope) + \n",
    "                                                                        '_min' for i in range(42)]), \n",
    "                          pd.DataFrame(data=(errtype_array_max - errtype_array_min), columns=['err_' + str(i+1) + \n",
    "                                                                                              '_move_' + str(scope) + \n",
    "                                                                                              '_diff' for i in range(42)]), \n",
    "                          pd.DataFrame(data=errtype_array_std, columns=['err_' + str(i+1) + \n",
    "                                                                        '_move_' + str(scope) + \n",
    "                                                                        '_std' for i in range(42)])], axis=1)\n",
    "    def qual_cnt(self) : # 퀄리티 단순합\n",
    "        id_q = self.df[['user_id','fwver',\"quality_0\",\"quality_1\",\"quality_2\",\"quality_3\",\"quality_4\",\"quality_5\",\"quality_6\",\"quality_7\",\"quality_8\",\"quality_9\",\"quality_10\",\"quality_11\",\"quality_12\"]].values\n",
    "        qual = np.zeros((self.num_user,41))\n",
    "        \n",
    "        # 0~ 27 : 펌웨어, 28~40: 퀄리티\n",
    "        for person_idx, fv, q0, q1, q2, q3, q4, q5, q6, q7, q8, q9, q10, q11, q12 in tqdm(id_q):\n",
    "\n",
    "            qual[person_idx - self.min_idx,28] += q0\n",
    "            qual[person_idx - self.min_idx,29] += q1\n",
    "            qual[person_idx - self.min_idx,30] += q2\n",
    "            qual[person_idx - self.min_idx,31] += q3\n",
    "            qual[person_idx - self.min_idx,32] += q4\n",
    "            qual[person_idx - self.min_idx,33] += q5\n",
    "            qual[person_idx - self.min_idx,34] += q6\n",
    "            qual[person_idx - self.min_idx,35] += q7\n",
    "            qual[person_idx - self.min_idx,36] += q8\n",
    "            qual[person_idx - self.min_idx,37] += q9\n",
    "            qual[person_idx - self.min_idx,38] += q10\n",
    "            qual[person_idx - self.min_idx,39] += q11\n",
    "            qual[person_idx - self.min_idx,40] += q12\n",
    "            \n",
    "        qual_plus = pd.DataFrame(data=qual, columns = ['err_'+str(i+1)+'_qual' for i in range(41)])\n",
    "        qual_plus = qual_plus.iloc[:,28:]\n",
    "        qual_plus.columns = ['err_'+str(i)+'_qual' for i in range(13)]\n",
    "        return qual_plus\n",
    "    \n",
    "    def make_id_in_qual(self): # 퀄리티 데이터에 없는 id 채우기\n",
    "        data = self.df\n",
    "        data2 = data.copy()\n",
    "        lst_id1 = sorted(list(data2.user_id.unique()))\n",
    "\n",
    "        for i in tqdm(range(self.min_idx, self.num_user +self.min_idx)):\n",
    "            if i in lst_id1:\n",
    "                pass\n",
    "            elif i not in lst_id1:\n",
    "                tmp_d = {'time': 20201101090000,\n",
    "                         'user_id': i,\n",
    "                         'fwver': '04.11.1384',\n",
    "                         'quality_0':0,'quality_1':0,'quality_2':0,'quality_3':0,'quality_4':0,'quality_5':0,\n",
    "                         'quality_6':0,'quality_7':0,'quality_8':0,'quality_9':0,'quality_10':0,'quality_11':0,\n",
    "                         'quality_12':0}\n",
    "                data2 = data2.append(tmp_d, ignore_index=True)\n",
    "\n",
    "        return data2\n",
    "    \n",
    "    def move_quality(self, data, number, moving=12): # 퀄리티 통계수치\n",
    "        tqdm.pandas()\n",
    "        \n",
    "\n",
    "        # max\n",
    "        max_data = data.groupby(['user_id'])['quality_' + str(number)].rolling(window=moving).max()\n",
    "        max_data = max_data.to_frame()\n",
    "        max_data = max_data.groupby(['user_id'])['quality_' + str(number)].max().to_frame().reset_index()\n",
    "        max_data = max_data.iloc[:,1:]\n",
    "        \n",
    "        # min\n",
    "        min_data = data.groupby(['user_id'])['quality_' + str(number)].rolling(window=moving).min()\n",
    "        min_data = min_data.to_frame()\n",
    "        min_data = min_data.groupby(['user_id'])['quality_' + str(number)].min().to_frame().reset_index()\n",
    "        min_data = min_data.iloc[:,1:]\n",
    "        \n",
    "        # std\n",
    "        std_data = data.groupby(['user_id'])['quality_' + str(number)].rolling(window=moving).std()\n",
    "        std_data = std_data.to_frame()\n",
    "        std_data = std_data.groupby(['user_id'])['quality_' + str(number)].mean().to_frame().reset_index()\n",
    "        std_data = std_data.iloc[:,1:]\n",
    "        \n",
    "        # mean\n",
    "        mean_data = data.groupby(['user_id'])['quality_' + str(number)].rolling(window=moving).mean()\n",
    "        mean_data = mean_data.to_frame()\n",
    "        mean_data = mean_data.groupby(['user_id'])['quality_' + str(number)].mean().to_frame().reset_index()\n",
    "        mean_data = mean_data.iloc[:,1:]\n",
    "\n",
    "        # concat\n",
    "        moving_result = pd.concat([max_data, min_data, std_data, mean_data], axis=1)\n",
    "        moving_result.columns = ['quality_'+str(number)+'_max',\n",
    "                                 'quality_'+str(number)+'_min', 'quality_'+str(number)+'_std',\n",
    "                                 'quality_'+str(number)+'_mean']\n",
    "\n",
    "        return moving_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = make_dataset2(train_err,3,15000,10000).move_errtype()\n",
    "train3 = make_dataset2(train_err,7,15000,10000).move_errtype()\n",
    "train4 = make_dataset2(qual1,7,15000,10000).make_qual_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test2 = make_dataset2(test_err,3,14999,30000).move_errtype()\n",
    "test3 = make_dataset2(test_err,7,14999,30000).move_errtype()\n",
    "test4 = make_dataset2(qual2,7,14999,30000).make_qual_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-3. make_dataset3\n",
    "  - run : 모든 함수 실행\n",
    "  - errtype_cnt : 전체 에러타입 발생횟수\n",
    "  - model_change : 모델의 변화\n",
    "  - fw_change : 펌웨어 버전의 변화\n",
    "  - errtype_sta : 전체 에러타입 발생횟수 관련 통계수치\n",
    "  - errtype_of_hour : 시간별 에러타입 통계수치\n",
    "  - errtype_of_day : 날짜별 에러타입 통계수치\n",
    "  - fwver_flows : 펌웨어버전 변경 패턴\n",
    "  - get_time_term : 시간 차이 \n",
    "  - connetion_err : connetion_err 에러코드 발생횟수\n",
    "  - fwver_time : 가자 오래 사용한 버전 관련 변수\n",
    "  - fw_err_cnt : 해당 버전과 에러타입 관련 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class mk_dataset3(): # train_err_data 관련 파생변수 생성 \n",
    "    \n",
    "    def __init__(self, df, types, num_user, num_errtype, first):\n",
    "        self.df = df\n",
    "        self.types = types\n",
    "        self.num_user = num_user\n",
    "        self.num_errtype = num_errtype\n",
    "        self.first = first\n",
    "        self.num_model_change =2\n",
    "        \n",
    "    def run(self):\n",
    "        vrb1 = self.errtype_cnt() # 에러타입마다 카운트\n",
    "        vrb2 =self.model_change() # 모델 변화\n",
    "        vrb3 =self.fw_change() # 펌웨어 버전 변화\n",
    "        m, std =self.errtype_sta() # 주별 에러타입 통계수치\n",
    "        mhour, stdhour, maxhour =self.errtype_of_hour() # 시간별 에러타입 통계수치\n",
    "        mday, stdday, maxday =self.errtype_of_day() # 일별 에러타입 통계수치\n",
    "        vrb4 =self.fwver_flows() # 펌웨어버전 연관성\n",
    "        \n",
    "        \n",
    "        \n",
    "        vrb5 =self.get_time_term() # 시간별 에러발생 상황\n",
    "        vrb6 =self.connetion_err() # connetion_err\n",
    "        vrb7 =self.fwver_time() # 펌웨어 버전 변경까지의 시간\n",
    "        vrb8=self.fw_err_cnt() # 주요 펌웨어 버전 카운트\n",
    "        \n",
    "        \n",
    "        return [vrb3,vrb2, vrb1, m, std ,mhour, stdhour, maxhour,mday, maxday, stdday, vrb4 , vrb5 ,vrb6  ,vrb7,vrb8]\n",
    "        \n",
    "    def errtype_cnt (self) : \n",
    "        errtypes = np.zeros((self.num_user,self.num_errtype))\n",
    "\n",
    "        for idx, value in tqdm(self.df[['user_id','errtype']].values):\n",
    "            errtypes[idx-self.first,value-1] +=1\n",
    "        return errtypes\n",
    "\n",
    "\n",
    "    def model_change(self): \n",
    "        df = self.df\n",
    "        first = self.first\n",
    "        \n",
    "        v=df[['user_id','model_nm']]\n",
    "        getdf =~(v == v.shift(1))\n",
    "        logical =(getdf.user_id.apply(int) + getdf.model_nm.apply(int)) > 0\n",
    "        df_model_counts =v[logical]\n",
    "\n",
    "\n",
    "        def get_model_change_id(num):# 모델이 변화한 유저\n",
    "            df_mc = df_model_counts.user_id.value_counts()\n",
    "            df_mc_user=list(df_mc.loc[df_mc ==num].to_frame().index)\n",
    "            sort_mc_user = df.loc[df.user_id.isin(df_mc_user)].drop_duplicates(\n",
    "                ['user_id','model_nm'],keep='last').drop_duplicates('user_id', keep='first').sort_values(\"time\").user_id\n",
    "            return sort_mc_user.values\n",
    "\n",
    "        one_m = get_model_change_id(1)\n",
    "        two_m = get_model_change_id(2)\n",
    "        thr_m = get_model_change_id(3)\n",
    "\n",
    "\n",
    "        model_n = np.zeros((self.num_user,self.num_model_change))\n",
    "        df['model_f'] =df['model_nm'].str[-1].astype('int')\n",
    "        one_df = df.loc[df.user_id.isin(one_m)][['user_id','model_f']].drop_duplicates().values\n",
    "        two_df =df.loc[df.user_id.isin(two_m)][['user_id','model_f']].drop_duplicates().reset_index(drop=True)\n",
    "        two_df['tf'] = two_df.index%2\n",
    "        two_df= two_df.pivot(index='user_id',columns='tf').reset_index().values\n",
    "        thr_df = df.loc[df.user_id.isin(thr_m)][['user_id','model_f']].drop_duplicates().values\n",
    "\n",
    "        for inx, value in tqdm(one_df):\n",
    "            model_n[inx-self.first,0]  +=value\n",
    "        for inx, value1,value2 in tqdm(two_df):\n",
    "            model_n[inx-self.first,0]  +=value1\n",
    "            model_n[inx-self.first,1]  +=value2\n",
    "\n",
    "        for inx, value in tqdm(thr_df):\n",
    "            model_n[inx-self.first,0]  +=value\n",
    "            \n",
    "        return model_n\n",
    "\n",
    "    def fw_change(self): \n",
    "        df= self.df\n",
    "        \n",
    "        fwver_total_dic ={}\n",
    "        for v in range(len(fwver_total)):\n",
    "            fwver_total_dic[sorted(list(fwver_total))[v]] = v+1\n",
    "\n",
    "\n",
    "        df['ver_num'] = df['fwver'].apply(lambda x : fwver_total_dic[x])\n",
    "        fwver_np = np.zeros((self.num_user,5))\n",
    "\n",
    "        v=df[['user_id','ver_num']]\n",
    "        getdf =~(v == v.shift(1))\n",
    "        logical =(getdf.user_id.apply(int) + getdf.ver_num.apply(int)) > 0\n",
    "        fwver_num=v[logical]\n",
    "        \n",
    "        fwver_num = fwver_num.reset_index(drop=True)\n",
    "        count =np.zeros(len(fwver_num),dtype=int)\n",
    "\n",
    "        for v in range(1,len(fwver_num)):\n",
    "            if fwver_num.user_id.values[v-1] ==fwver_num.user_id.values[v]:\n",
    "                count[v] = count[v-1] +1\n",
    "\n",
    "\n",
    "        fwver_num['count'] =count\n",
    "        fw_v = fwver_num.loc[fwver_num['count'].isin([0,1,2,3,4])].pivot(\n",
    "            index='user_id',columns='count').reset_index().fillna(0).values\n",
    "        fw_v =fw_v.astype('int64')\n",
    "\n",
    "        for inx, v1,v2,v3,v4,v5 in tqdm(fw_v):\n",
    "            fwver_np[inx-self.first,0] =v1\n",
    "            fwver_np[inx-self.first,1] =v2\n",
    "            fwver_np[inx-self.first,2] =v3\n",
    "            fwver_np[inx-self.first,3] =v4\n",
    "            fwver_np[inx-self.first,4] =v5\n",
    "        return fwver_np\n",
    "\n",
    "    def errtype_sta(self):\n",
    "        df = self.df\n",
    "        \n",
    "        df['week'] =self.df.time.dt.isocalendar().week\n",
    "\n",
    "        df = df.loc[(df.time >=pd.to_datetime('2020-11-01 00:00:00')) & (df.time<= pd.to_datetime('2020-11-30 23:59:59'))]\n",
    "        datas = df[['user_id','errtype','week']]\n",
    "        df_=datas[['user_id','week','errtype']].value_counts().to_frame().reset_index()\n",
    "        df_ =df_.sort_values(['user_id','week']).rename(columns = {0:'counts'}).reset_index(drop=True)\n",
    "\n",
    "        df1 =df_.loc[df_.week ==44][['user_id','errtype','counts']].values\n",
    "        df2 =df_.loc[df_.week ==45][['user_id','errtype','counts']].values\n",
    "        df3 =df_.loc[df_.week ==46][['user_id','errtype','counts']].values\n",
    "        df4 =df_.loc[df_.week ==47][['user_id','errtype','counts']].values\n",
    "        df5 =df_.loc[df_.week ==48][['user_id','errtype','counts']].values\n",
    "\n",
    "        day_data = np.zeros((self.num_user,42,5))\n",
    "        for i, dfa in enumerate([df1,df2,df3,df4,df5]):\n",
    "            for inx , val1 ,val2 in tqdm(dfa):\n",
    "                day_data[:,:,i][inx-self.first,val1-1] = val2\n",
    "\n",
    "        m=day_data.mean(axis=2)\n",
    "        std=day_data.std(axis=2)    \n",
    "        return m, std\n",
    "        \n",
    "    def errtype_of_hour (self):\n",
    "        df = self.df\n",
    "        df['hour'] =df.time.dt.hour\n",
    "\n",
    "\n",
    "        df = df.loc[(df.time >=pd.to_datetime('2020-11-01 00:00:00')) & (df.time<= pd.to_datetime('2020-11-30 23:59:59'))]\n",
    "        datas = df[['user_id','errtype','hour']]\n",
    "        df_=datas[['user_id','hour','errtype']].value_counts().to_frame().reset_index()\n",
    "        df_ =df_.sort_values(['user_id','hour']).rename(columns = {0:'counts'}).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        day_data = np.zeros((self.num_user,42,24))\n",
    "        for i in range(24):\n",
    "            dfa = df_.loc[df_['hour']==i][['user_id','errtype','counts']].values\n",
    "            for inx , val1 ,val2 in tqdm(dfa):\n",
    "                day_data[:,:,i][inx-self.first,val1-1] = val2\n",
    "\n",
    "        mhour=day_data.mean(axis=2)\n",
    "        stdhour=day_data.std(axis=2)       \n",
    "        maxhour=day_data.max(axis=2)\n",
    "        return mhour, stdhour, maxhour\n",
    " \n",
    "    def errtype_of_day (self):\n",
    "        df = self.df\n",
    "        \n",
    "        df['day'] =df.time.dt.day\n",
    "\n",
    "\n",
    "        df = df.loc[(df.time >=pd.to_datetime('2020-11-01 00:00:00')) & (df.time<= pd.to_datetime('2020-11-30 23:59:59'))]\n",
    "        datas = df[['user_id','errtype','day']]\n",
    "        df_=datas[['user_id','day','errtype']].value_counts().to_frame().reset_index()\n",
    "        df_ =df_.sort_values(['user_id','day']).rename(columns = {0:'counts'}).reset_index(drop=True)\n",
    "\n",
    "        day_data = np.zeros((self.num_user,42,30))\n",
    "        for i in range(30):\n",
    "            dfa = df_.loc[df_['day']==(i+1)][['user_id','errtype','counts']].values\n",
    "            for inx , val1 ,val2 in tqdm(dfa):\n",
    "                day_data[:,:,i][inx-self.first,val1-1] = val2\n",
    "\n",
    "        mday=day_data.mean(axis=2)\n",
    "        stdday=day_data.std(axis=2)       \n",
    "        maxday=day_data.max(axis=2) \n",
    "        return mday, stdday, maxday\n",
    "    \n",
    "    \n",
    "    def fwver_flows(self):\n",
    "        target_df = self.df\n",
    "        first_num = self.first\n",
    "        count_num =self.num_user\n",
    "\n",
    "        dp = target_df[['user_id','model_nm','fwver']]\n",
    "        unique_data =target_df[(dp !=dp.shift(1)).sum(axis=1)>0]\n",
    "\n",
    "        dp2 = target_df[['user_id','model_nm']]\n",
    "        unique_data2 =target_df[(dp2 !=dp2.shift(1)).sum(axis=1)>0]\n",
    "\n",
    "        fwver_total_dic ={}\n",
    "        for v in range(len(fwver_total)):\n",
    "            fwver_total_dic[sorted(list(fwver_total))[v]] = v+1\n",
    "\n",
    "\n",
    "        fwver = np.zeros((count_num,24))\n",
    "        for idx in tqdm(unique_data.user_id.unique()):\n",
    "            df_md =unique_data2.loc[unique_data2.user_id==idx].model_nm.values\n",
    "            df_fw = unique_data.loc[unique_data.user_id==idx].fwver.values\n",
    "\n",
    "            for md in range(len(df_md)):\n",
    "                fwver[idx-first_num,md] = int(df_md[md][-1])+1\n",
    "\n",
    "            for l in range(3,len(df_fw)+3):\n",
    "                fwver[idx-first_num,l] =fwver_total_dic[df_fw[l-3]]\n",
    "\n",
    "        fw_df = pd.DataFrame(fwver).reset_index().rename(columns={'index':'user_id'})\n",
    "\n",
    "        fwver_total_dic_rev = {v: k for k, v in fwver_total_dic.items()}\n",
    "        fwver_total_dic_rev2 = fwver_total_dic_rev.copy()\n",
    "        fwver_total_dic_rev[0] =0\n",
    "        fwver_total_dic_rev2[0] = '04.22.1750'\n",
    "\n",
    "\n",
    "        fw_df[3] =fw_df[3].apply(lambda x : fwver_total_dic_rev2[x])\n",
    "        for _ in range(4,8):\n",
    "            fw_df[_] =fw_df[_].apply(lambda x : fwver_total_dic_rev[x])\n",
    "\n",
    "\n",
    "        fw_df = fw_df.rename(columns={0:'md1',1:'md2',2:'md3',3:'fw1',4:'fw2',5:'fw3',6:'fw4',7:'fw5'})\n",
    "        fw_df['user_id'] =fw_df['user_id']+10000\n",
    "\n",
    "        pre_df=fw_df.iloc[:,:9]\n",
    "\n",
    "        md_flow = {str(x.astype(\"int\")):(i+1) for i,x in enumerate(\n",
    "            pre_df[['md1','md2','md3']].drop_duplicates().reset_index(drop=True).values)}\n",
    "        \n",
    "        fw_flow = {str(x):(i+1) for i,x in enumerate(\n",
    "            pre_df[['fw1','fw2','fw3','fw4','fw5']].drop_duplicates().reset_index(drop=True).values)}\n",
    "\n",
    "\n",
    "        def fw_change_counter(x):# 버전 변화 횟수\n",
    "            fwlst = []\n",
    "            for v in ['fw1','fw2','fw3','fw4','fw5']:\n",
    "                if x[v] ==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    fwlst +=[x[v]]\n",
    "            if len(fwlst) ==len(list(set(fwlst))):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        def mean_str_fw_dum(x):# 평균\n",
    "            fwlst = []\n",
    "            for v in ['fw1','fw2','fw3','fw4','fw5']:\n",
    "                if x[v] ==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    fwlst +=[int(x[v].replace('.',\"\"))]\n",
    "            return np.array(fwlst).mean()\n",
    "\n",
    "        def std_str_fw_dum(x): # 표준편차\n",
    "            fwlst = []\n",
    "            for v in ['fw1','fw2','fw3','fw4','fw5']:\n",
    "                if x[v] ==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    fwlst +=[int(x[v].replace('.',\"\"))]\n",
    "            return np.array(fwlst).std()\n",
    "\n",
    "        pre_df=fw_df.iloc[:,:9]\n",
    "        pre_df['md_counts'] = pre_df[['md1','md2','md3']].astype('bool').sum(axis=1)\n",
    "        pre_df['fw_counts'] = pre_df[['fw1','fw2','fw3','fw4','fw5']].astype('bool').sum(axis=1)\n",
    "\n",
    "        pre_df['fw_change'] = pre_df.apply(fw_change_counter,axis=1)\n",
    "        pre_df['fw_flows'] = pre_df.apply(lambda x : fw_flow[str(x[['fw1','fw2','fw3','fw4','fw5']].values)],axis=1)\n",
    "        pre_df['md_flows'] = pre_df.apply(lambda x :md_flow[str(x[['md1','md2','md3']].values.astype(\"int\"))],axis=1)\n",
    "        pre_df['fw_mean'] = pre_df.apply(mean_str_fw_dum,axis=1)\n",
    "        pre_df['fw_std'] = pre_df.apply(std_str_fw_dum,axis=1)\n",
    "\n",
    "\n",
    "        fw_model_flow =pre_df.iloc[:,9:].values \n",
    "        return fw_model_flow\n",
    "    \n",
    "    def get_time_term(self):\n",
    "        df = self.df\n",
    "        first_num = self.first\n",
    "        count_num = self.num_user\n",
    "\n",
    "        time_term = np.zeros((count_num,4))\n",
    "        tmp =df[['user_id','time']].drop_duplicates()\n",
    "\n",
    "        for v in tqdm(tmp.user_id.unique()):\n",
    "            test =tmp.loc[tmp.user_id ==v].time\n",
    "            if len(test) <=2:\n",
    "                time_term[v-first_num,0] = 0\n",
    "                time_term[v-first_num,1] = 0\n",
    "                time_term[v-first_num,2] = test.values[-1]-test.values[0]\n",
    "                time_term[v-first_num,3] = len(test)\n",
    "            else:\n",
    "                time_term[v-first_num,0] = (test -test.shift(1)).max().total_seconds()\n",
    "                time_term[v-first_num,1] = (test -test.shift(1)).min().total_seconds()\n",
    "                time_term[v-first_num,2] = test.values[-1]-test.values[0]  \n",
    "                time_term[v-first_num,3] = len(test)\n",
    "\n",
    "        dft = pd.DataFrame(time_term).copy()\n",
    "\n",
    "        dft[0] =dft[0]/3600\n",
    "        dft[2] =dft[2]/3600/24/10e8\n",
    "        dft[2] =np.where(dft[2].values==0,1,dft[2].values)\n",
    "        dft[5] =dft[0]/dft[3]\n",
    "        dft[5] = dft[0]/dft[3]*3600\n",
    "        dft[6] = dft[0]/24/dft[2]\n",
    "        return dft.fillna(0).values\n",
    "        \n",
    "    def connetion_err(self):\n",
    "        first_num = self.first\n",
    "        count_num = self.num_user\n",
    "        \n",
    "        err_df =self.df[['user_id','time','errcode']].dropna(axis=0)\n",
    "\n",
    "        df_con =err_df.loc[err_df.errcode.str.contains(\"connection\")]\n",
    "        df_con['check_time'] = df_con.time.dt.date\n",
    "        df_con['check_hour'] =df_con.time.dt.hour\n",
    "\n",
    "        def connetion_df(con_df):\n",
    "            day_con_err =con_df[['user_id','check_time']].value_counts().groupby(\"user_id\").max()\n",
    "            hour_con_err=con_df.groupby(['user_id','check_time','check_hour']).size().groupby(\"user_id\").max()\n",
    "            tenmin_con_err =con_df.set_index('time').groupby(['user_id','errcode']).resample(\"10min\").size().groupby(\"user_id\").max()\n",
    "            con_trans =pd.concat([day_con_err,hour_con_err,tenmin_con_err],axis=1)\n",
    "            return con_trans\n",
    "\n",
    "        total_conn_err=[connetion_df(df_con)]\n",
    "        for errs in tqdm(['connection timeout', 'connection fail to establish','connectionterminated by local host',\n",
    "                          'connection fail for LMP response timout','L2CAP connection cancelled']):\n",
    "            \n",
    "            con_esta = df_con.loc[df_con.errcode.str.contains(errs)]\n",
    "            total_conn_err.append(connetion_df(con_esta))\n",
    "\n",
    "        base_df =pd.DataFrame(range(first_num,first_num+count_num)).rename(columns={0:'user_id'}).set_index('user_id')\n",
    "\n",
    "        connetion_err_pre = pd.concat(total_conn_err+[base_df],axis=1).fillna(0).values\n",
    "        return connetion_err_pre\n",
    "    \n",
    "    def fwver_time(self):\n",
    "        \n",
    "        fwver_total_dic ={}\n",
    "        for v in range(len(fwver_total)):\n",
    "            fwver_total_dic[sorted(list(fwver_total))[v]] = v+1\n",
    "            \n",
    "        tsed = self.df.dropna(axis=0).reset_index(drop=True)[['user_id','time','fwver']]\n",
    "        dfw = tsed[['user_id','fwver']]\n",
    "        fw_d =dfw.loc[(dfw !=dfw.shift(1)).sum(axis=1)>0]\n",
    "\n",
    "        main_fw_ar = np.zeros((self.num_user,6))\n",
    "        for i,tgid in enumerate(tqdm(range(self.first,self.first+self.num_user))):\n",
    "\n",
    "            tgdf =fw_d.loc[fw_d.user_id ==tgid].iloc[1:,:]\n",
    "            tgidtotal = tsed.loc[tsed.user_id ==tgid]\n",
    "            \n",
    "            try:\n",
    "                data =tgidtotal.loc[sorted([tgidtotal.index[0]] + [x-1 for x in tgdf.index]+[x for x in tgdf.index] + [tgidtotal.index[-1]] )]\n",
    "                t1 =data.time\n",
    "                time_delta = (t1-t1.shift(1)).dt.total_seconds()\n",
    "\n",
    "                main_fwver =data.loc[time_delta.loc[time_delta==time_delta.max()].index].fwver.values[0]\n",
    "                main_fw_ar[i,0] = fwver_total_dic[main_fwver]\n",
    "                main_fw_ar[i,1] =(time_delta[1::2].values).max().astype('float')/(time_delta.values[1:]).sum().astype('float')  #target fw workingtime / total\n",
    "                if len(time_delta) ==1:\n",
    "                    main_fw_ar[i,2] =0  \n",
    "                    main_fw_ar[i,3] =0  \n",
    "                    main_fw_ar[i,4] =0  \n",
    "                    main_fw_ar[i,5] =0  \n",
    "                else:\n",
    "                    main_fw_ar[i,2] =time_delta[::2].min()/3600 \n",
    "                    main_fw_ar[i,3] =time_delta[::2].std()/3600\n",
    "                    main_fw_ar[i,4] =time_delta[1::2].values.astype('float').std()/3600 \n",
    "                    main_fw_ar[i,5] =(time_delta[1::2].values.astype('float')/3600).var()  \n",
    "            except:\n",
    "                main_fw_ar[i,0] =0\n",
    "                main_fw_ar[i,1] =0\n",
    "                main_fw_ar[i,2] =0\n",
    "                main_fw_ar[i,3] =0\n",
    "                main_fw_ar[i,4] =0\n",
    "                main_fw_ar[i,5] =0\n",
    "                \n",
    "        return main_fw_ar\n",
    "    \n",
    "    def fw_err_cnt(self):\n",
    "        fwver_total_dic ={}\n",
    "        for v in range(len(fwver_total)):\n",
    "            fwver_total_dic[sorted(list(fwver_total))[v]] = v+1\n",
    "            \n",
    "        tred = self.df.dropna(axis=0).reset_index(drop=True)[['user_id','time','fwver']]\n",
    "        tred_all = self.df.dropna(axis=0).reset_index(drop=True)\n",
    "        \n",
    "        tsed = self.df.dropna(axis=0).reset_index(drop=True)[['user_id','time','fwver']]\n",
    "        dfw = tsed[['user_id','fwver']]\n",
    "        fw_d =dfw.loc[(dfw !=dfw.shift(1)).sum(axis=1)>0]\n",
    "\n",
    "        main_fw_err_counts = np.zeros((self.num_user,84))\n",
    "        for i,tgid in enumerate(tqdm(range(self.first,self.first+self.num_user))):\n",
    "\n",
    "            tgdf =fw_d.loc[fw_d.user_id ==tgid].iloc[1:,:]\n",
    "            tgidtotal = tred_all.loc[tred_all.user_id ==tgid]\n",
    "\n",
    "            \n",
    "            try:\n",
    "                data =tgidtotal.loc[sorted([tgidtotal.index[0]] + [x-1 for x in tgdf.index]+[x for x in tgdf.index] + [tgidtotal.index[-1]] )]\n",
    "                t1 =data.time\n",
    "                time_delta = (t1-t1.shift(1)).dt.total_seconds()\n",
    "\n",
    "                main_fwver =data.loc[time_delta.loc[time_delta==time_delta.max()].index].fwver.values[0]\n",
    "\n",
    "\n",
    "                main_date = tgidtotal.loc[tgidtotal.fwver ==main_fwver][['time','errtype']]\n",
    "                main_date['date'] =main_date.time.dt.date\n",
    "                main_time_del  =(main_date.time.dt.date.values[-1] -main_date.time.dt.date.values[0]).days\n",
    "                date_first= main_date.time.dt.date.values[0]\n",
    "                err_count_main = np.zeros((42,main_time_del+1))\n",
    "                for n in  range(main_time_del+1):\n",
    "                    lcdf = main_date.loc[main_date.date ==date_first]\n",
    "                    for errtype in lcdf.errtype.values:\n",
    "                        err_count_main[errtype-1,n] +=1\n",
    "                    date_first += dt.timedelta(days=1)\n",
    "\n",
    "                main_fw_err_counts[i,:42] =     err_count_main.mean(axis=1)\n",
    "                main_fw_err_counts[i,42:] =     err_count_main.std(axis=1)\n",
    "\n",
    "            except:\n",
    "                main_fw_err_counts[i,:] = 0\n",
    "                \n",
    "        return main_fw_err_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names (df):\n",
    "    col_dict = dict()\n",
    "    \n",
    "    for i in range(5) : # 펌웨어 버전 변화\n",
    "        col_dict[str(i)] = 'fw' + str(i+1)  \n",
    "    for i in range(5,7):# 모델 변화\n",
    "        col_dict[str(i)] = 'model' + str(i-4)\n",
    "        \n",
    "    for i in range(49,91) :# 주별 에러타입 평균\n",
    "        col_dict[str(i)] = 'mean_week_errtype' + str(i-48)\n",
    "          \n",
    "    for i in range(133,175): # 시간당 에러타입 평균\n",
    "        col_dict[str(i)] = 'mean_hour_errtype' + str(i - 132)\n",
    "    for i in range(175,217): # 시간당 에러타입 표준편차\n",
    "        col_dict[str(i)] = 'std_hour_errtype' + str(i - 174)\n",
    "    for i in range(217,259): # 시간당 에러타입 최댓값\n",
    "        col_dict[str(i)] = 'max_hour_errtype' + str(i - 216)\n",
    "        \n",
    "    for i in range(259,301): # 일별 에러타입 평균\n",
    "        col_dict[str(i)] = 'mean_day_errtype' + str(i - 260)\n",
    "    for i in range(301,343): # 일별 에러타입 최댓값\n",
    "        col_dict[str(i)] = 'max_day_errtype' + str(i - 133)\n",
    "    for i in range(343,385): # 일별 에러타입 표준편차\n",
    "        col_dict[str(i)] = 'std_day_errtype' + str(i - 342)\n",
    "    \n",
    "    for i in range(385,392): # 펌웨어 버전 흐름\n",
    "        col_dict[str(i)] = 'fw_ver_flow' + str(i - 384)\n",
    "    for i in range(392,398): # 버전 사용 기간\n",
    "        col_dict[str(i)] = 'fw_ver_term' + str(i - 391)\n",
    "    for i in range(398,416): # connection errcode\n",
    "        col_dict[str(i)] = 'connection_' + str(i - 397)\n",
    "        \n",
    "    for i in range(416,422): # 주 사용 fwver\n",
    "        col_dict[str(i)] = 'main_ver_' + str(i - 415)\n",
    "        \n",
    "    for i in range(422,506): # fw_errtype\n",
    "        col_dict[i] = 'fw_errtype' + str(i - 421)\n",
    "    return df.rename(columns = col_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5 = mk_dataset3(train_err2,'train',15000,42,10000).run()\n",
    "train5 = pd.DataFrame(np.concatenate(tuple(train5),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test5 = mk_dataset3(test_err2,'test',14999,42,30000).run()\n",
    "test5 = pd.DataFrame(np.concatenate(tuple(test5),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4. 최종 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_concat(df1,df2,df3,df4,df5) :# 변수 선택 및 데이터프레임 병합\n",
    "    select1 = df5.iloc[ : , 0:8]\n",
    "    select2 = df5.iloc[ : , 50:92]\n",
    "    select3 = df5.iloc[ : , 134:]\n",
    "\n",
    "    final_df = pd.concat([df1, df2, df3, df4,\n",
    "                         select1, select2, select3], axis=1)\n",
    "    \n",
    "    for i in final_df.columns:\n",
    "        str_i = str(i)\n",
    "        if 'diff' in str_i :\n",
    "            final_df.drop(i, axis='columns', inplace=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = select_concat(train1, train2, train3, train4 ,train5)\n",
    "test_final = select_concat(test1, test2, test3,test4, test5)\n",
    "\n",
    "train_final.to_csv(SAVE_PATH + \"./trainf.csv\", index = False)\n",
    "test_final.to_csv(SAVE_PATH + \"./testf.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 훈련 및 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_final.isnull().sum().sum())\n",
    "print(train_final.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_final = pd.read_csv(SAVE_PATH + \"./trainf.csv\")\n",
    "# test_final = pd.read_csv(SAVE_PATH + \"./testf.csv\")\n",
    "\n",
    "# 결측치가 많은 칼럼 처리\n",
    "del train_final[419]\n",
    "del train_final[418]\n",
    "train_final[417] = train_final[417].fillna(1.000000)\n",
    "\n",
    "del test_final[419]\n",
    "del test_final[418]\n",
    "test_final[417] = test_final[417].fillna(1.000000)\n",
    "test_final = test_final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y값 생성\n",
    "label = np.zeros(15000)\n",
    "label[train_problem.user_id.unique()-10000] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_final.copy()\n",
    "train_y = label.astype(int)\n",
    "\n",
    "# 카테고리 변수 지정\n",
    "train_x['ver'] = train_x['ver'].astype('category')\n",
    "train_x['ver']= train_x['ver'].cat.codes\n",
    "\n",
    "test_final['ver'] = test_final['ver'].astype('category')\n",
    "test_final['ver']= test_final['ver'].cat.codes\n",
    "\n",
    "(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 듀얼 블렌딩\n",
    "  - 1. 주요 파라미터 및 최적값들을 찾음.\n",
    "  - 2. 두 최적값 각각을 사용하는 두 모델을 생성 후 학습\n",
    "  - 3. 최종 예측 결과값을 블렌딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터가 다른 두 캣부스트 모델을 동시에 학습후 블렌딩\n",
    "\n",
    "def dual_blending (train_x, train_y):\n",
    "    \n",
    "    models1    = []\n",
    "    auc_scores1   = []\n",
    "    models2    = []\n",
    "    auc_scores2   = []\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    # 5 Kfold cross validation\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in k_fold.split(train_x,train_y):\n",
    "\n",
    "        X = train_x.iloc[train_idx,:]\n",
    "        y = train_y[train_idx]\n",
    "        valid_x = train_x.iloc[val_idx,:]\n",
    "        valid_y = train_y[val_idx]\n",
    "\n",
    "        # 첫번째 모델\n",
    "        model1 = CatBoostRegressor(iterations=1000, l2_leaf_reg = 0.01, random_seed = 1422)\n",
    "        model1.fit(X,y, cat_features = ['ver'])\n",
    "\n",
    "\n",
    "        valid_prob1 = model1.predict(valid_x)\n",
    "        valid_pred1 = np.where(valid_prob1 > threshold, 1, 0)\n",
    "        auc_score1 = roc_auc_score(   valid_y, valid_prob1)\n",
    "\n",
    "\n",
    "        models1.append(model1)\n",
    "        auc_scores1.append(auc_score1)\n",
    "\n",
    "        print('==========================================================')\n",
    "\n",
    "        # 두번째 모델\n",
    "        model2 = CatBoostRegressor(iterations=1000, l2_leaf_reg = 0.003, random_seed = 1422)\n",
    "        model2.fit(X,y, cat_features = ['ver'])\n",
    "\n",
    "\n",
    "        valid_prob2 = model2.predict(valid_x)\n",
    "        valid_pred2 = np.where(valid_prob2 > threshold, 1, 0)\n",
    "        auc_score2 = roc_auc_score(valid_y, valid_prob2)\n",
    "\n",
    "\n",
    "        models2.append(model2)\n",
    "        auc_scores2.append(auc_score2)\n",
    "\n",
    "        print('==========================================================')\n",
    "        \n",
    "    return models1, models2, auc_scores1, auc_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group1, group2, scores1, scores2 = dual_blending(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측 및 제출파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "pred_y_list1 = []\n",
    "pred_y_list2 = []\n",
    "\n",
    "for m1 in group1:\n",
    "    pred_y1 = m1.predict(test_final)\n",
    "    pred_y_list1.append(pred_y1.reshape(-1,1))\n",
    "    \n",
    "for m2 in group2:    \n",
    "    pred_y2 = m2.predict(test_final)\n",
    "    pred_y_list2.append(pred_y2.reshape(-1,1))\n",
    "    \n",
    "pred1 = np.mean(pred_y_list1, axis = 0)\n",
    "pred2 = np.mean(pred_y_list2, axis = 0)\n",
    "(pred1,pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_ensemble = 0.5 * pred1 + 0.5 * pred2\n",
    "pred_ensemble\n",
    "\n",
    "sample_submssion = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "sample_submssion['problem'] = pred_ensemble.reshape(-1)\n",
    "sample_submssion.to_csv(RESULT_PATH + \"Final_Sub.csv\", index = False)\n",
    "sample_submssion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
